<h1>MarketKnowledge</h1>

Scrape thousands of Reddit comments in one go and turn them into fast, useful insights‚Äîsentiment, themes, and keywords; so you can validate markets and understand customers without reading threads for hours.

MarketKnowledge = Reddit data ‚Üí structured insights for:
	‚Ä¢	Market research & validation
	‚Ä¢	Voice-of-customer mining (phrases, objections, desires)
	‚Ä¢	Rapid keyword/theme discovery for copy & SEO

‚ú® Features
	‚Ä¢	Bulk scraping of posts and comments from subreddits, threads, or keyword searches
	‚Ä¢	Sentiment & keyword extraction per comment (configurable)
	‚Ä¢	Collections & saved items to organize research
	‚Ä¢	CSV export for spreadsheets or further analysis
	‚Ä¢	Auth & per-user data (Supabase)
	‚Ä¢	Cooldown / rate-limit guards to avoid hammering Reddit


üß± Architecture

/backend         # FastAPI (Python) ‚Äì scraping + API
/frontend/...    # Next.js/React (TypeScript) ‚Äì web UI
Supabase         # Auth + Postgres database + storage
Reddit (PRAW)    # Data source API

	‚Ä¢	Backend: FastAPI + PRAW (Reddit). Writes to Supabase/Postgres.
	‚Ä¢	Frontend: Next.js app that authenticates via Supabase and calls the backend.
	‚Ä¢	DB: Supabase hosts Postgres + Auth. Tables store users‚Äô scrapes, comments, collections, etc.

<h2>üí™ Try It Out! </h2>

<p>This app is hosted at marketknowledge.app
Feel free to check it out without needing to clone and set it up yourself!</p> 
(I don't know if I did a good job writing the setup instructions anyways, so it might save you unnecessary headaches to visit the site!)

‚∏ª

üöÄ Quickstart

Prerequisites
	‚Ä¢	Node 18+ and npm (or pnpm)
	‚Ä¢	Python 3.10+ and pip
	‚Ä¢	A Supabase project (free is fine)
	‚Ä¢	A Reddit app (for PRAW)
	‚Ä¢	Optionally: an LLM key (e.g., OpenAI) if you want AI-powered sentiment/keywords

You can run frontend and backend locally with just env files‚Äîno Docker needed.

‚∏ª

‚öôÔ∏è Supabase Setup
	1.	Create a new project at https://supabase.com/
	2.	Grab your Project URL, anon key, and service_role key from Project Settings ‚Üí API.
	3.	Create tables. In Supabase SQL editor, run:

# Users are handled by Supabase Auth; profile table optional
create table if not exists profiles (
  id uuid primary key references auth.users(id) on delete cascade,
  created_at timestamp with time zone default now()
);

# Track each scrape job a user runs
create table if not exists scrapes (
  id bigint generated by default as identity primary key,
  user_id uuid references auth.users(id) on delete cascade,
  query text not null,                       -- subreddit, url, or keyword
  status text not null default 'completed',  -- queued|running|completed|failed
  total_fetched int default 0,
  created_at timestamp with time zone default now()
);

# Core: comments we‚Äôve ingested
create table if not exists reddit_comments (
  id bigint generated by default as identity primary key,
  user_id uuid references auth.users(id) on delete cascade,
  scrape_id bigint references scrapes(id) on delete cascade,
  reddit_id text,              -- the thing id
  body text,
  author text,
  score int,
  permalink text,
  subreddit text,
  created_utc timestamptz,
  sentiment text,              -- e.g., positive|neutral|negative
  keywords text[],             -- array of extracted tokens
  inserted_at timestamptz default now()
);

# User-defined buckets
create table if not exists collections (
  id bigint generated by default as identity primary key,
  user_id uuid references auth.users(id) on delete cascade,
  name text not null,
  created_at timestamptz default now(),
  unique (user_id, name)
);

# Many-to-many: comments users saved to a collection
create table if not exists collection_comments (
  collection_id bigint references collections(id) on delete cascade,
  comment_id bigint references comments(id) on delete cascade,
  primary key (collection_id, comment_id)
);

# (Optional) rate-limiting record
create table if not exists user_limits (
  user_id uuid primary key references auth.users(id) on delete cascade,
  last_scrape_at timestamptz
);

# RLS policies (starter set)
alter table scrapes enable row level security;
alter table comments enable row level security;
alter table collections enable row level security;
alter table collection_comments enable row level security;

# Users can only see their own data
create policy "scrapes by owner" on scrapes
  for select using (auth.uid() = user_id);

create policy "scrapes insert by owner" on scrapes
  for insert with check (auth.uid() = user_id);

create policy "comments by owner" on comments
  for select using (auth.uid() = user_id);

create policy "comments insert by owner" on comments
  for insert with check (auth.uid() = user_id);

create policy "collections by owner" on collections
  for select using (auth.uid() = user_id);

create policy "collections write by owner" on collections
  for insert with check (auth.uid() = user_id);

create policy "collection_comments by owner" on collection_comments
  for select using (exists (
    select 1 from collections c
    where c.id = collection_id and c.user_id = auth.uid()
  ));

create policy "collection_comments write by owner" on collection_comments
  for insert with check (exists (
    select 1 from collections c
    where c.id = collection_id and c.user_id = auth.uid()
  ));

If your backend uses the service_role key to write, you may keep RLS on and still insert‚Äîservice role bypasses RLS. The frontend must use the anon key.

‚∏ª

üîë Reddit (PRAW) App
	1.	Go to https://www.reddit.com/prefs/apps
	2.	Click create app ‚Üí choose script
	3.	Note client id, client secret
	4.	Set a custom user agent, e.g. marketknowledge-app/1.0 by your_reddit_username

‚∏ª

üîê Environment Variables

You‚Äôll have two env files‚Äîone for the backend (Python), one for the frontend (Next.js).

Backend: backend/.env

# Supabase (backend should use SERVICE role for server-side writes)
SUPABASE_URL=...            # https://xyzcompany.supabase.co
SUPABASE_SERVICE_ROLE_KEY=...

# Reddit (PRAW)
REDDIT_CLIENT_ID=...
REDDIT_CLIENT_SECRET=...
REDDIT_USERNAME=...
REDDIT_PASSWORD=...
REDDIT_USER_AGENT=marketknowledge-app/1.0 by <your_username>

# Optional: AI for sentiment/keywords (if your backend uses OpenAI or similar)
OPENAI_API_KEY=...
MODEL=gpt-4o-mini           # or your model of choice

# Tuning / safety
SCRAPE_COOLDOWN_SECONDS=30
MAX_COMMENTS_PER_SCRAPE=5000

If you‚Äôre using OAuth refresh tokens instead of username/password, swap those vars accordingly.

Frontend: frontend/market-knowledge-frontend/.env.local

NEXT_PUBLIC_SUPABASE_URL=...
NEXT_PUBLIC_SUPABASE_ANON_KEY=...

# How the web app reaches your backend API
NEXT_PUBLIC_API_BASE_URL=http://localhost:8000


‚∏ª

üõ†Ô∏è Local Development

1) Backend (FastAPI)

cd backend
python -m venv .venv
# macOS/Linux:
source .venv/bin/activate
# Windows (PowerShell):
# .venv\Scripts\Activate.ps1

pip install -r requirements.txt
# or: pip install uvicorn fastapi praw supabase
cp .env.example .env  # if present, else create .env per above
uvicorn app.main:app --reload --port 8000

The API should now be live at http://localhost:8000
Docs (if enabled): http://localhost:8000/docs

Common endpoints (example)

Names may differ‚Äîadjust to your codebase.

	‚Ä¢	POST /scrape ‚Äî start a scrape ({ "query": "r/SomeSubreddit" } or { "url": "...reddit.com/..." })
	‚Ä¢	GET  /comments ‚Äî list your ingested comments (supports filters like ?scrape_id=...)
	‚Ä¢	GET  /export.csv ‚Äî download current filtered results as CSV

2) Frontend (Next.js)

cd frontend/market-knowledge-frontend
npm install
cp .env.local.example .env.local  # if present
npm run dev

Open http://localhost:3000. Sign up/in via Supabase auth UI, then run scrapes from the app.

‚∏ª

üß™ Usage Tips
	‚Ä¢	Queries you can scrape:
	‚Ä¢	Subreddit: r/<name> (fetch top/new/hot‚Äîdepends on your backend logic)
	‚Ä¢	Thread URL: fetch comments for a single post
	‚Ä¢	Keyword search: subreddit + keyword (if implemented in backend)
	‚Ä¢	Respect Reddit limits. The app enforces a cooldown (default 30s). Don‚Äôt run multiple huge scrapes at once.
	‚Ä¢	Exports: Use CSV to take analysis elsewhere (Sheets, Python notebooks, etc.).

‚∏ª

üìù Configuration & Extensibility
	‚Ä¢	Sentiment/Keywords
	‚Ä¢	If you enabled an LLM, tweak the model & prompt in backend code.
	‚Ä¢	For deterministic keywording, consider spaCy/YAKE/RapidFuzz as alternatives or fallbacks.
	‚Ä¢	Rate limiting
	‚Ä¢	Adjust SCRAPE_COOLDOWN_SECONDS and (optionally) write the last scrape time to user_limits.
	‚Ä¢	Collections
	‚Ä¢	Collections are per-user (unique(user_id, name)) to avoid name collisions across users.

‚∏ª

üîí Security Notes
	‚Ä¢	Keep the service role key strictly on the server (backend only).
	‚Ä¢	Frontend should only use anon key.
	‚Ä¢	Never commit .env files.
	‚Ä¢	If deploying, set env vars in your host (Vercel/Render/Fly/AWS/etc.).

‚∏ª

‚òÅÔ∏è Deployment (suggested)
	‚Ä¢	Frontend: Vercel (import Next.js app).
	‚Ä¢	Backend: Render/Fly.io/AWS Lightsail/EC2 with uvicorn or gunicorn.
	‚Ä¢	Database: Supabase (managed Postgres).
	‚Ä¢	Set NEXT_PUBLIC_API_BASE_URL to your backend‚Äôs public URL.

‚∏ª

üß∞ Scripts (examples)

Back end:

# dev server
uvicorn app.main:app --reload --port 8000
# prod (example)
gunicorn -k uvicorn.workers.UvicornWorker app.main:app --bind 0.0.0.0:8000

Front end:

npm run dev
npm run build
npm start


‚∏ª

üêû Troubleshooting
	‚Ä¢	401/403 from Supabase: Using anon key on the server by accident? Use SERVICE_ROLE_KEY in backend.
	‚Ä¢	‚ÄúFailed to fetch‚Äù on frontend: Check NEXT_PUBLIC_API_BASE_URL and CORS settings in FastAPI. Sometimes you need to create a new personal script since they will expire randomly from my experience. 
	‚Ä¢	Reddit auth failures: Verify app type is script, user agent is set, credentials are correct.

‚∏ª

ü§ù Contributing

PRs welcome! Good first issues:
	‚Ä¢	Add more filters (date range, min score, author)
	‚Ä¢	Advanced exports (JSONL, Parquet)
	‚Ä¢	Additional analysis (topic modeling, clustering)
	‚Ä¢	UI polish and empty states

Please open an issue before large refactors.

‚∏ª

üôå Credits

Built by @MartinWang17. Thanks to Reddit, Supabase, and ChatGPT for all the help tutoring on each part of the code!

‚∏ª
